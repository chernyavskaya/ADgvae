{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import tqdm\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import inspect\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, DataLoader, DataListLoader\n",
    "from torch_geometric.nn import EdgeConv, global_mean_pool, DataParallel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data,Dataset\n",
    "from torch_scatter import scatter_mean, scatter\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import MetaLayer, EdgeConv, global_mean_pool, DynamicEdgeConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Samples\n",
    "DATA_PATH = '/eos/cms/store/group/phys_b2g/CASE/h5_files/full_run2/BB_UL_MC_small_v2/'\n",
    "\n",
    "TRAIN_NAME = 'BB_batch0.h5'\n",
    "filename_bg = DATA_PATH + TRAIN_NAME \n",
    "batch_size = 128\n",
    "train_set_size = int((5*10e3//batch_size)*batch_size)\n",
    "#file_bg = h5py.File(filename_bg, 'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "deta_jj = 1.4\n",
    "jPt = 400\n",
    "\n",
    "def xyze_to_eppt(constituents):\n",
    "    ''' converts an array [N x 100, 4] of particles\n",
    "from px, py, pz, E to eta, phi, pt (mass omitted)\n",
    "    '''\n",
    "    PX, PY, PZ, E = range(4)\n",
    "    pt = np.sqrt(np.float_power(constituents[:,PX], 2) + np.float_power(constituents[:,PY], 2), dtype='float32') # numpy.float16 dtype -> float power to avoid overflow\n",
    "    eta = np.arcsinh(np.divide(constituents[:,PZ], pt, out=np.zeros_like(pt), where=pt!=0.), dtype='float32')\n",
    "    phi = np.arctan2(constituents[:,PY], constituents[:,PX], dtype='float32')\n",
    "\n",
    "    return np.stack([pt, eta, phi], axis=1)\n",
    "\n",
    "side = True\n",
    "to_train = True\n",
    "\n",
    "datas = []\n",
    "for i_e in range(1000):\n",
    "    if to_train: \n",
    "        if file_bg['truth_label'][i_e]!=0 : #train only on QCD\n",
    "            continue \n",
    "    if side :\n",
    "        if not (file_bg[\"jet_kinematics\"][i_e,1] > deta_jj):\n",
    "            continue\n",
    "    else : \n",
    "        if not (file_bg[\"jet_kinematics\"][i_e,1] < deta_jj):\n",
    "            continue\n",
    "    for i_j in range(2): #each event has 2 jets\n",
    "        pf_cands = np.array(file_bg[\"jet{}_PFCands\".format(i_j+1)][i_e])\n",
    "        pf_pt_eta_phi = xyze_to_eppt(pf_cands)\n",
    "        n_particles = int(np.sum(pf_pt_eta_phi[:,0]!=0)) #if pt!=0\n",
    "        particles = np.zeros((n_particles, 7)) #px,py,pz,E, pt, eta, phi = 7\n",
    "        #particles = np.dstack((pf_cands[0:n_particles,:],np.array(pf_pt_eta_phi[0:n_particles,:])))\n",
    "        particles = np.hstack((pf_cands[0:n_particles,:],np.array(pf_pt_eta_phi[0:n_particles,:])))\n",
    "        pairs = np.stack([[m, n] for (m, n) in itertools.product(range(n_particles),range(n_particles)) if m!=n])\n",
    "        edge_index = torch.tensor(pairs, dtype=torch.long)\n",
    "        edge_index=edge_index.t().contiguous()\n",
    "        # save particles as node attributes and target\n",
    "        x = torch.tensor(particles, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        datas.append([data])\n",
    "datas = sum(datas,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 10\n",
      "train subset dim: 40\n",
      "validation subset dim 10\n",
      "train_dataloader dim: 8\n",
      "val dataloader dim: 2\n"
     ]
    }
   ],
   "source": [
    "def get_present_constit(x,n):\n",
    "    return x[0:n,:] \n",
    "\n",
    "def concat_features(feats_1,feats_2):\n",
    "    return np.hstack((feats_1[:,:],feats_2[:,:]))\n",
    "\n",
    "class GraphDataset(Dataset):  ####pytorch Dataset\n",
    "    def __init__(self, root, transform=None, pre_transform=None,\n",
    "                 n_events=-1,n_jets=10e3, side_reg=1, features='xyzeptep',n_proc=1):\n",
    "        \"\"\"\n",
    "        Initialize parameters of graph dataset\n",
    "        Args:\n",
    "            root (str): dir path\n",
    "            n_events (int): how many events to process (-1=all in a file (there is a max))\n",
    "            n_jets (int) : how many total jets to use\n",
    "            side_reg (bool):true or false, side region for training, otherwise for testing on signal \n",
    "            n_proc (int): number of processes to split into\n",
    "            features (str): (px, py, pz) or relative (pt, eta, phi)\n",
    "        \"\"\"\n",
    "        max_events = int(1.1e6)\n",
    "        self.n_events = max_events if n_events==-1 else n_events\n",
    "        self.n_jets = n_jets \n",
    "        self.side_reg = side_reg\n",
    "        self.n_proc = n_proc\n",
    "        self.chunk_size = self.n_events // self.n_proc\n",
    "        self.features = features\n",
    "        self.dEtaJJ = 1.4\n",
    "        self.jPt = 400\n",
    "        self.jet_kin_names = ['mJJ', 'DeltaEtaJJ', 'j1Pt', 'j1Eta', 'j1Phi',\\\n",
    "                                        'j1M', 'j2Pt', 'j2Eta', 'j2Phi', 'j2M', 'j3Pt', 'j3Eta', 'j3Phi', 'j3M']\n",
    "        self.pf_kin_names = ['px','py','pz','E']\n",
    "        \n",
    "        super(GraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        self.input_files = sorted(glob.glob(self.raw_dir+'/*.root'))\n",
    "        return [f.split('/')[-1] for f in self.input_files]\n",
    "\n",
    "    def __len__(self):\n",
    "      #  return len(self.n_jets) \n",
    "        return 50\n",
    "  \n",
    "    def xyze_to_ptep(self,constituents):\n",
    "        ''' converts an array [N x 100, 4] of particles\n",
    "from px, py, pz, E to eta, phi, pt (mass omitted)\n",
    "    '''\n",
    "        PX = self.pf_kin_names.index('px')\n",
    "        PY = self.pf_kin_names.index('py')\n",
    "        PZ = self.pf_kin_names.index('pz')\n",
    "        E = self.pf_kin_names.index('E')\n",
    "        pt = np.sqrt(np.float_power(constituents[:,:,PX], 2) + np.float_power(constituents[:,:,PY], 2), dtype='float32') # numpy.float16 dtype -> float power to avoid overflow\n",
    "        eta = np.arcsinh(np.divide(constituents[:,:,PZ], pt, out=np.zeros_like(pt), where=pt!=0.), dtype='float32')\n",
    "        phi = np.arctan2(constituents[:,:,PY], constituents[:,:,PX], dtype='float32')\n",
    "        return np.stack([pt, eta, phi], axis=2)\n",
    "\n",
    "\n",
    "\n",
    "    def read_events(self,idx):\n",
    "        \n",
    "        #Data Samples\n",
    "        DATA_PATH = '/eos/cms/store/group/phys_b2g/CASE/h5_files/full_run2/BB_UL_MC_small_v2/'\n",
    "        TRAIN_NAME = 'BB_batch0.h5'\n",
    "        filename_bg = DATA_PATH + TRAIN_NAME \n",
    "        in_file = h5py.File(filename_bg, 'r') \n",
    "        jet_kin = np.array(in_file[\"jet_kinematics\"])[0:1000]\n",
    "        truth = np.array(in_file[\"truth_label\"])[0:1000]\n",
    "\n",
    "        j1Pt_mask = (jet_kin[:,self.jet_kin_names.index('j1Pt')] > self.jPt)\n",
    "        j2Pt_mask = (jet_kin[:,self.jet_kin_names.index('j2Pt')] > self.jPt)\n",
    "        full_mask = j1Pt_mask & j2Pt_mask\n",
    "        if self.side_reg : \n",
    "            full_mask = full_mask & (jet_kin[:,self.jet_kin_names.index('DeltaEtaJJ')] > self.dEtaJJ)\n",
    "        else : \n",
    "            full_mask = full_mask & (jet_kin[:,self.jet_kin_names.index('DeltaEtaJJ')] < self.dEtaJJ)\n",
    "\n",
    "        #Apply mask on jet kinematics, truth and pf cands\n",
    "        jet_kin = jet_kin[full_mask]\n",
    "        truth = truth[full_mask]\n",
    "        jet_const = [np.array(in_file[\"jet1_PFCands\"][0:1000])[full_mask],np.array(in_file[\"jet2_PFCands\"][0:1000])[full_mask]]\n",
    "                \n",
    "\n",
    "        pf_out_list = []\n",
    "        jet_prop_list = []\n",
    "\n",
    "        for i_j in range(2): #each event has 2 jets\n",
    "            pf_xyze = jet_const[i_j]\n",
    "            pf_ptep = self.xyze_to_ptep(pf_xyze)\n",
    "            n_particles = np.sum(pf_xyze[:,:,self.pf_kin_names.index('E')]!=0,axis=1) #E is 3rd \n",
    "            pf_xyze_out = list(map(get_present_constit,pf_xyze,n_particles))\n",
    "            pf_ptep_out = list(map(get_present_constit,pf_ptep,n_particles))\n",
    "            pf_tot_out = list(map(concat_features,pf_xyze_out,pf_ptep_out))\n",
    "            pf_out_list.append(pf_tot_out)\n",
    "\n",
    "            n_jet_feats = 6\n",
    "            jet_prop = np.zeros((len(pf_tot_out),n_jet_feats))\n",
    "            jet_prop[:,0] = n_particles\n",
    "            for i_f,f_name in enumerate('M,Pt,Eta,Phi'.split(',')):\n",
    "                jet_prop[:,i_f+1] = jet_kin[:,self.jet_kin_names.index('j{}{}'.format(i_j+1,f_name))]\n",
    "            jet_prop[:,n_jet_feats-1] = truth[:,0]\n",
    "            jet_prop_list.append(jet_prop)\n",
    "            \n",
    "        #return list of pf particles, and list of global jet properties\n",
    "        return sum(pf_out_list, []),np.vstack((jet_prop_list[0],jet_prop_list[1]))      \n",
    "                 \n",
    "\n",
    "    def get(self,idx):\n",
    "        '''Yields one data graph'''\n",
    "        pf_cands, jet_prop = self.read_events(idx)   \n",
    "        \n",
    "        i_evt = idx\n",
    "        #for i_evt in range(len(pf_cands)):\n",
    "        n_particles = pf_cands[i_evt].shape[0]\n",
    "        print('event : ',i_evt,' n particle : ',n_particles)\n",
    "        pairs = np.stack([[m, n] for (m, n) in itertools.product(range(n_particles),range(n_particles)) if m!=n])\n",
    "        edge_index = torch.tensor(pairs, dtype=torch.long)\n",
    "        edge_index=edge_index.t().contiguous()\n",
    "        # save particles as node attributes and target\n",
    "        x = torch.tensor(pf_cands[i_evt], dtype=torch.float)\n",
    "        u = torch.tensor(jet_prop[i_evt,:], dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index,u=torch.unsqueeze(u, 0))\n",
    "        print('data.x.n_particles ',n_particles)\n",
    "        return data\n",
    "        \n",
    "data_dir = '/eos/user/n/nchernya/MLHEP/AnomalyDetection/ADgvae/output_models/pytroch/'\n",
    "dataset = GraphDataset(root=data_dir)\n",
    "    \n",
    "validation_split = 0.2\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "if dataset_size > 2:\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "else: \n",
    "    split = 1\n",
    "print(dataset_size,split)\n",
    "random_seed= 1001\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(dataset, [dataset_size - split, split],\n",
    "                                                             generator=torch.Generator().manual_seed(random_seed))\n",
    "print(\"train subset dim:\", len(train_subset))\n",
    "print(\"validation subset dim\", len(val_subset))\n",
    "dataloaders = {\n",
    "    'train':  DataLoader(train_subset, batch_size=5, shuffle=True),\n",
    "    'val':   DataLoader(val_subset, batch_size=5, shuffle=True)\n",
    "    }\n",
    "print(\"train_dataloader dim:\", len(dataloaders['train']))\n",
    "print(\"val dataloader dim:\", len(dataloaders['val']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "event :  0  n particle :  46\n",
      "data.x.n_particles  46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 46.0000,  14.0078, 636.5000,  -1.6787,   1.2065,   0.0000]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get(0).u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Model definitions.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_scatter import scatter_mean, scatter\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import MetaLayer, EdgeConv, global_mean_pool, DynamicEdgeConv\n",
    "\n",
    "\n",
    "# GNN AE using EdgeConv (mean aggregation graph operation). Basic GAE model.\n",
    "class EdgeNet(nn.Module):\n",
    "    def __init__(self, input_dim=7, output_dim=4, big_dim=32, hidden_dim=2, aggr='mean'):\n",
    "        super(EdgeNet, self).__init__()\n",
    "        encoder_nn = nn.Sequential(nn.Linear(2*(input_dim), big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        decoder_nn = nn.Sequential(nn.Linear(2*(hidden_dim), big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm1d(input_dim)\n",
    "\n",
    "        self.encoder = EdgeConv(nn=encoder_nn,aggr=aggr)\n",
    "        self.decoder = EdgeConv(nn=decoder_nn,aggr=aggr)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.batchnorm(data.x)\n",
    "        x = self.encoder(x,data.edge_index)\n",
    "        x = self.decoder(x,data.edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader, total, batch_size, loss_ftn_obj):\n",
    "    model.train()\n",
    "\n",
    "    sum_loss = 0.\n",
    "    t = tqdm.tqdm(enumerate(loader),total=total/batch_size)\n",
    "    for i,data in t:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss, batch_output = forward_loss(model, data, loss_ftn_obj, device, multi_gpu=False)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = batch_loss.item()\n",
    "        sum_loss += batch_loss\n",
    "        t.set_description('train loss = %.7f' % batch_loss)\n",
    "        t.refresh() # to show immediately the update\n",
    "\n",
    "    return sum_loss / (i+1)\n",
    "\n",
    "\n",
    "# helper to perform correct loss\n",
    "def forward_loss(model, data, loss_ftn_obj, device, multi_gpu=False):\n",
    "    \n",
    "    if not multi_gpu:\n",
    "        data = data.to(device)\n",
    "\n",
    "    if 'emd_loss' in loss_ftn_obj.name or loss_ftn_obj.name == 'chamfer_loss' or loss_ftn_obj.name == 'hungarian_loss':\n",
    "        batch_output = model(data)\n",
    "        if multi_gpu:\n",
    "            data = Batch.from_data_list(data).to(device)\n",
    "        y = data.x\n",
    "        batch = data.batch\n",
    "        batch_loss = loss_ftn_obj.loss_ftn(batch_output, y, batch)\n",
    "\n",
    "    elif loss_ftn_obj.name == 'emd_in_forward':\n",
    "        _, batch_loss = model(data)\n",
    "        batch_loss = batch_loss.mean()\n",
    "\n",
    "    elif loss_ftn_obj.name == 'vae_loss':\n",
    "        batch_output, mu, log_var = model(data)\n",
    "        y = torch.cat([d.x for d in data]).to(device) if multi_gpu else data.x\n",
    "        y = y.contiguous()\n",
    "        batch_loss = loss_ftn_obj.loss_ftn(batch_output, y, mu, log_var)\n",
    "\n",
    "    else:\n",
    "        batch_output = model(data)\n",
    "        y = torch.cat([d.x for d in data]).to(device) if multi_gpu else data.x\n",
    "        y = y.contiguous()\n",
    "        batch_loss = loss_ftn_obj.loss_ftn(batch_output, y)\n",
    "\n",
    "    return batch_loss, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "multi_gpu = False #torch.cuda.device_count()>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        :param data: torch tensor\n",
    "        \"\"\"\n",
    "        self.mean = torch.mean(data, dim=0)\n",
    "        self.std = torch.std(data, dim=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data, log_pt=False):\n",
    "        \"\"\"\n",
    "        :param data: torch tensor\n",
    "        :param log_pt: undo log transformation on pt\n",
    "        \"\"\"\n",
    "        inverse = (data * self.std) + self.mean\n",
    "        if log_pt:\n",
    "            inverse[:,0] = (10 ** inverse[:,0]) - 1\n",
    "        return inverse\n",
    "\n",
    "def standardize(train_dataset,log_pt=False):\n",
    "    \"\"\"\n",
    "    standardize dataset and return scaler for inversion\n",
    "    :param train_dataset: list of Data objects\n",
    "    :param valid_dataset: list of Data objects\n",
    "    :param test_dataset: list of Data objects\n",
    "    :param log_pt: log pt before standardization\n",
    "    :return scaler: sklearn StandardScaler\n",
    "    \"\"\"\n",
    "    train_x = torch.cat([d.x for d in train_dataset])\n",
    "    if log_pt:\n",
    "        train_x[:,0] = torch.log(train_x[:,0] + 1)\n",
    "\n",
    "    scaler = Standardizer()\n",
    "    scaler.fit(train_x)\n",
    "    for d in train_dataset:\n",
    "        d.x[:,:] = scaler.transform(d.x)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = DataLoader(datas, batch_size=128)\n",
    "#scaler = standardize(datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyze_to_ptetaphi_torch(y):\n",
    "    ''' converts an array [N x 100, 4] of particles\n",
    "from px, py, pz, E to pt,eta, phi\n",
    "    '''\n",
    "    PX, PY, PZ, E = range(4)\n",
    "    pt = torch.sqrt(torch.pow(y[:,PX], 2) + torch.pow(y[:,PY], 2)) \n",
    "    eta = torch.asinh(torch.where(pt < 10e-5, torch.zeros_like(pt), torch.div(y[:,PZ], pt)))\n",
    "    phi = torch.atan2(y[:,PY], y[:,PX])\n",
    "\n",
    "    relu =  m = nn.ReLU() #inplace=True\n",
    "    y_E_trimmed = relu(y[:,-1]) #trimming E\n",
    "    y_pt_trimmed = relu(pt) #trimming pt\n",
    "    full_y = torch.stack((y[:,0],y[:,1],y[:,2],y_E_trimmed,y_pt_trimmed,eta,phi), dim=1)\n",
    "\n",
    "    return full_y\n",
    "\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, lossname, device=torch.device('cuda:0')):\n",
    "        loss = getattr(self, lossname)\n",
    "        self.name = lossname\n",
    "        self.loss_ftn = loss\n",
    "        self.device = device\n",
    "    def mse(self, x, y):\n",
    "        return F.mse_loss(x, y, reduction='mean')\n",
    "    \n",
    "    def mse_coordinates(self, y,x): #for some reason convension is : out,in\n",
    "        #From px,py,pz,E get pt, eta, phi (do not predict them)\n",
    "        #x is px,py,pz,E,pt,eta,phi\n",
    "        #y is px,py,pz,E\n",
    "        full_y = xyze_to_ptetaphi_torch(y)\n",
    "        return self.mse(x,full_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeNet(\n",
       "  (batchnorm): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (encoder): EdgeConv(nn=Sequential(\n",
       "    (0): Linear(in_features=14, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (5): ReLU()\n",
       "  ))\n",
       "  (decoder): EdgeConv(nn=Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
       "  ))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss\n",
    "loss_ftn_obj = LossFunction('mse_coordinates', device=device)\n",
    "\n",
    "# model\n",
    "input_dim = 7\n",
    "output_dim = 4\n",
    "big_dim = 32\n",
    "hidden_dim = 2\n",
    "model = EdgeNet(input_dim=input_dim,output_dim=output_dim, big_dim=big_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 10e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, threshold=1e-6)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "event :  16  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  10  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  18  n particle :  32\n",
      "data.x.n_particles  32\n",
      "getting data\n",
      "event :  13  n particle :  72\n",
      "data.x.n_particles  72\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 616.6954956:   0%|          | 0/10.0 [00:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 616.6954956:   0%|          | 0/10.0 [00:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 616.6954956:  10%|█         | 1/10.0 [00:04<00:42,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  4  n particle :  51\n",
      "data.x.n_particles  51\n",
      "getting data\n",
      "event :  0  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n",
      "event :  29  n particle :  38\n",
      "data.x.n_particles  38\n",
      "getting data\n",
      "event :  38  n particle :  60\n",
      "data.x.n_particles  60\n",
      "getting data\n",
      "event :  26  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 767.9191895:  10%|█         | 1/10.0 [00:08<00:42,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 767.9191895:  10%|█         | 1/10.0 [00:08<00:42,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 767.9191895:  20%|██        | 2/10.0 [00:08<00:36,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  48  n particle :  31\n",
      "data.x.n_particles  31\n",
      "getting data\n",
      "event :  19  n particle :  63\n",
      "data.x.n_particles  63\n",
      "getting data\n",
      "event :  5  n particle :  25\n",
      "data.x.n_particles  25\n",
      "getting data\n",
      "event :  43  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n",
      "event :  40  n particle :  87\n",
      "data.x.n_particles  87\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 324.1234741:  20%|██        | 2/10.0 [00:12<00:36,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 324.1234741:  20%|██        | 2/10.0 [00:12<00:36,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 324.1234741:  30%|███       | 3/10.0 [00:12<00:30,  4.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  6  n particle :  88\n",
      "data.x.n_particles  88\n",
      "getting data\n",
      "event :  44  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n",
      "event :  23  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  28  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  36  n particle :  39\n",
      "data.x.n_particles  39\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 841.8758545:  30%|███       | 3/10.0 [00:16<00:30,  4.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 841.8758545:  30%|███       | 3/10.0 [00:16<00:30,  4.37s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 841.8758545:  40%|████      | 4/10.0 [00:16<00:25,  4.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  31  n particle :  66\n",
      "data.x.n_particles  66\n",
      "getting data\n",
      "event :  27  n particle :  75\n",
      "data.x.n_particles  75\n",
      "getting data\n",
      "event :  46  n particle :  75\n",
      "data.x.n_particles  75\n",
      "getting data\n",
      "event :  34  n particle :  65\n",
      "data.x.n_particles  65\n",
      "getting data\n",
      "event :  21  n particle :  45\n",
      "data.x.n_particles  45\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 487.0435486:  40%|████      | 4/10.0 [00:21<00:25,  4.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 487.0435486:  40%|████      | 4/10.0 [00:21<00:25,  4.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 487.0435486:  50%|█████     | 5/10.0 [00:21<00:21,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  12  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n",
      "event :  49  n particle :  35\n",
      "data.x.n_particles  35\n",
      "getting data\n",
      "event :  41  n particle :  93\n",
      "data.x.n_particles  93\n",
      "getting data\n",
      "event :  1  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  47  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 238.5124207:  50%|█████     | 5/10.0 [00:30<00:21,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 238.5124207:  50%|█████     | 5/10.0 [00:30<00:21,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 238.5124207:  60%|██████    | 6/10.0 [00:30<00:23,  5.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  35  n particle :  40\n",
      "data.x.n_particles  40\n",
      "getting data\n",
      "event :  33  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  15  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  30  n particle :  66\n",
      "data.x.n_particles  66\n",
      "getting data\n",
      "event :  9  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 247.4612122:  60%|██████    | 6/10.0 [00:34<00:23,  5.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 247.4612122:  60%|██████    | 6/10.0 [00:34<00:23,  5.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 247.4612122:  70%|███████   | 7/10.0 [00:34<00:16,  5.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  2  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n",
      "event :  32  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n",
      "event :  42  n particle :  37\n",
      "data.x.n_particles  37\n",
      "getting data\n",
      "event :  3  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n",
      "event :  17  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2223.4409180:  70%|███████   | 7/10.0 [00:38<00:16,  5.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2223.4409180:  70%|███████   | 7/10.0 [00:38<00:16,  5.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2223.4409180:  80%|████████  | 8/10.0 [00:38<00:10,  5.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  25  n particle :  67\n",
      "data.x.n_particles  67\n",
      "Epoch: 00, Training Loss:   718.3840\n",
      "getting data\n",
      "event :  13  n particle :  72\n",
      "data.x.n_particles  72\n",
      "getting data\n",
      "event :  30  n particle :  66\n",
      "data.x.n_particles  66\n",
      "getting data\n",
      "event :  10  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  0  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1232.3421631:   0%|          | 0/10.0 [00:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1232.3421631:   0%|          | 0/10.0 [00:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1232.3421631:  10%|█         | 1/10.0 [00:04<00:38,  4.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  42  n particle :  37\n",
      "data.x.n_particles  37\n",
      "getting data\n",
      "event :  29  n particle :  38\n",
      "data.x.n_particles  38\n",
      "getting data\n",
      "event :  4  n particle :  51\n",
      "data.x.n_particles  51\n",
      "getting data\n",
      "event :  46  n particle :  75\n",
      "data.x.n_particles  75\n",
      "getting data\n",
      "event :  41  n particle :  93\n",
      "data.x.n_particles  93\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1289.0974121:  10%|█         | 1/10.0 [00:08<00:38,  4.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1289.0974121:  10%|█         | 1/10.0 [00:08<00:38,  4.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1289.0974121:  20%|██        | 2/10.0 [00:08<00:34,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  17  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  27  n particle :  75\n",
      "data.x.n_particles  75\n",
      "getting data\n",
      "event :  40  n particle :  87\n",
      "data.x.n_particles  87\n",
      "getting data\n",
      "event :  48  n particle :  31\n",
      "data.x.n_particles  31\n",
      "getting data\n",
      "event :  34  n particle :  65\n",
      "data.x.n_particles  65\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 344.1509705:  20%|██        | 2/10.0 [00:12<00:34,  4.34s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 344.1509705:  20%|██        | 2/10.0 [00:12<00:34,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 344.1509705:  30%|███       | 3/10.0 [00:12<00:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  16  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  38  n particle :  60\n",
      "data.x.n_particles  60\n",
      "getting data\n",
      "event :  49  n particle :  35\n",
      "data.x.n_particles  35\n",
      "getting data\n",
      "event :  2  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n",
      "event :  19  n particle :  63\n",
      "data.x.n_particles  63\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 343.2242432:  30%|███       | 3/10.0 [00:17<00:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 343.2242432:  30%|███       | 3/10.0 [00:17<00:30,  4.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 343.2242432:  40%|████      | 4/10.0 [00:17<00:26,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  18  n particle :  32\n",
      "data.x.n_particles  32\n",
      "getting data\n",
      "event :  47  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n",
      "event :  6  n particle :  88\n",
      "data.x.n_particles  88\n",
      "getting data\n",
      "event :  23  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  3  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 651.8779297:  40%|████      | 4/10.0 [00:22<00:26,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 651.8779297:  40%|████      | 4/10.0 [00:22<00:26,  4.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 651.8779297:  50%|█████     | 5/10.0 [00:22<00:22,  4.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  35  n particle :  40\n",
      "data.x.n_particles  40\n",
      "getting data\n",
      "event :  15  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  36  n particle :  39\n",
      "data.x.n_particles  39\n",
      "getting data\n",
      "event :  25  n particle :  67\n",
      "data.x.n_particles  67\n",
      "getting data\n",
      "event :  12  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 523.3646851:  50%|█████     | 5/10.0 [00:26<00:22,  4.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 523.3646851:  50%|█████     | 5/10.0 [00:26<00:22,  4.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 523.3646851:  60%|██████    | 6/10.0 [00:26<00:17,  4.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  21  n particle :  45\n",
      "data.x.n_particles  45\n",
      "getting data\n",
      "event :  1  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  31  n particle :  66\n",
      "data.x.n_particles  66\n",
      "getting data\n",
      "event :  28  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  44  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 296.9815979:  60%|██████    | 6/10.0 [00:30<00:17,  4.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 296.9815979:  60%|██████    | 6/10.0 [00:30<00:17,  4.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 296.9815979:  70%|███████   | 7/10.0 [00:30<00:13,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  26  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n",
      "event :  5  n particle :  25\n",
      "data.x.n_particles  25\n",
      "getting data\n",
      "event :  9  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  43  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n",
      "event :  32  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 642.2234497:  70%|███████   | 7/10.0 [00:35<00:13,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 642.2234497:  70%|███████   | 7/10.0 [00:35<00:13,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 642.2234497:  80%|████████  | 8/10.0 [00:35<00:08,  4.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  33  n particle :  44\n",
      "data.x.n_particles  44\n",
      "Epoch: 01, Training Loss:   665.4078\n",
      "getting data\n",
      "event :  1  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  42  n particle :  37\n",
      "data.x.n_particles  37\n",
      "getting data\n",
      "event :  29  n particle :  38\n",
      "data.x.n_particles  38\n",
      "getting data\n",
      "event :  28  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 748.1409912:   0%|          | 0/10.0 [00:03<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 748.1409912:   0%|          | 0/10.0 [00:03<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 748.1409912:  10%|█         | 1/10.0 [00:03<00:35,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  40  n particle :  87\n",
      "data.x.n_particles  87\n",
      "getting data\n",
      "event :  9  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  15  n particle :  54\n",
      "data.x.n_particles  54\n",
      "getting data\n",
      "event :  6  n particle :  88\n",
      "data.x.n_particles  88\n",
      "getting data\n",
      "event :  31  n particle :  66\n",
      "data.x.n_particles  66\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 271.3630066:  10%|█         | 1/10.0 [00:07<00:35,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 271.3630066:  10%|█         | 1/10.0 [00:07<00:35,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 271.3630066:  20%|██        | 2/10.0 [00:07<00:31,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  3  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n",
      "event :  5  n particle :  25\n",
      "data.x.n_particles  25\n",
      "getting data\n",
      "event :  18  n particle :  32\n",
      "data.x.n_particles  32\n",
      "getting data\n",
      "event :  2  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n",
      "event :  16  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 570.7133179:  20%|██        | 2/10.0 [00:11<00:31,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 570.7133179:  20%|██        | 2/10.0 [00:11<00:31,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 570.7133179:  30%|███       | 3/10.0 [00:11<00:27,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  44  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n",
      "event :  27  n particle :  75\n",
      "data.x.n_particles  75\n",
      "getting data\n",
      "event :  41  n particle :  93\n",
      "data.x.n_particles  93\n",
      "getting data\n",
      "event :  0  n particle :  46\n",
      "data.x.n_particles  46\n",
      "getting data\n",
      "event :  26  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 331.3677063:  30%|███       | 3/10.0 [00:15<00:27,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 331.3677063:  30%|███       | 3/10.0 [00:15<00:27,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 331.3677063:  40%|████      | 4/10.0 [00:15<00:23,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  19  n particle :  63\n",
      "data.x.n_particles  63\n",
      "getting data\n",
      "event :  12  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n",
      "event :  25  n particle :  67\n",
      "data.x.n_particles  67\n",
      "getting data\n",
      "event :  48  n particle :  31\n",
      "data.x.n_particles  31\n",
      "getting data\n",
      "event :  46  n particle :  75\n",
      "data.x.n_particles  75\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 339.2796936:  40%|████      | 4/10.0 [00:19<00:23,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 339.2796936:  40%|████      | 4/10.0 [00:19<00:23,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 339.2796936:  50%|█████     | 5/10.0 [00:19<00:19,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  33  n particle :  44\n",
      "data.x.n_particles  44\n",
      "getting data\n",
      "event :  17  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  30  n particle :  66\n",
      "data.x.n_particles  66\n",
      "getting data\n",
      "event :  10  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  35  n particle :  40\n",
      "data.x.n_particles  40\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1885.3748779:  50%|█████     | 5/10.0 [00:23<00:19,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1885.3748779:  50%|█████     | 5/10.0 [00:23<00:19,  3.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1885.3748779:  60%|██████    | 6/10.0 [00:23<00:15,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  38  n particle :  60\n",
      "data.x.n_particles  60\n",
      "getting data\n",
      "event :  13  n particle :  72\n",
      "data.x.n_particles  72\n",
      "getting data\n",
      "event :  34  n particle :  65\n",
      "data.x.n_particles  65\n",
      "getting data\n",
      "event :  36  n particle :  39\n",
      "data.x.n_particles  39\n",
      "getting data\n",
      "event :  21  n particle :  45\n",
      "data.x.n_particles  45\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 344.7800293:  60%|██████    | 6/10.0 [00:27<00:15,  3.97s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 344.7800293:  60%|██████    | 6/10.0 [00:27<00:15,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 344.7800293:  70%|███████   | 7/10.0 [00:27<00:11,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  47  n particle :  59\n",
      "data.x.n_particles  59\n",
      "getting data\n",
      "event :  23  n particle :  24\n",
      "data.x.n_particles  24\n",
      "getting data\n",
      "event :  4  n particle :  51\n",
      "data.x.n_particles  51\n",
      "getting data\n",
      "event :  32  n particle :  52\n",
      "data.x.n_particles  52\n",
      "getting data\n",
      "event :  49  n particle :  35\n",
      "data.x.n_particles  35\n",
      "getting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1141.5419922:  70%|███████   | 7/10.0 [00:31<00:11,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1141.5419922:  70%|███████   | 7/10.0 [00:31<00:11,  3.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 1141.5419922:  80%|████████  | 8/10.0 [00:31<00:08,  4.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event :  43  n particle :  46\n",
      "data.x.n_particles  46\n",
      "Epoch: 02, Training Loss:   704.0702\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n_epochs = 3\n",
    "stale_epochs = 0\n",
    "loss = 999999\n",
    "train_losses = []\n",
    "for epoch in range(0, n_epochs):\n",
    "    #loss = train(model, optimizer, loader, len(datas), 128, loss_ftn_obj)\n",
    "    loss = train(model, optimizer, dataloaders['train'], 50, 5, loss_ftn_obj)\n",
    "    train_losses.append(loss)\n",
    "    print('Epoch: {:02d}, Training Loss:   {:.4f}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gen_in_out(model, loader, device):\n",
    "    model.eval()\n",
    "    input_fts = []\n",
    "    reco_fts = []\n",
    "\n",
    "    for t in loader:\n",
    "        if isinstance(t, list):\n",
    "            for d in t:\n",
    "                input_fts.append(d.x)\n",
    "        else:\n",
    "            input_fts.append(t.x)\n",
    "            t.to(device)\n",
    "\n",
    "        reco_out = model(t)\n",
    "        if isinstance(reco_out, tuple):\n",
    "            reco_out = reco_out[0]\n",
    "        reco_fts.append(reco_out.cpu().detach())\n",
    "\n",
    "    input_fts = torch.cat(input_fts)\n",
    "    reco_fts = torch.cat(reco_fts)\n",
    "    return input_fts, reco_fts\n",
    "\n",
    "def plot_reco_for_loader(model, loader, device, scaler, inverse_scale, model_fname, save_dir, feature_format):\n",
    "    input_fts, reco_fts = gen_in_out(model, loader, device)\n",
    "    if inverse_scale:\n",
    "        input_fts = scaler.inverse_transform(input_fts)\n",
    "        reco_fts = scaler.inverse_transform(reco_fts)\n",
    "    plot_reco_difference(input_fts, reco_fts, model_fname, save_dir, feature_format)\n",
    "\n",
    "    \n",
    "def plot_reco_difference(input_fts, reco_fts, model_fname, save_path, feature='hadronic'):\n",
    "    \"\"\"\n",
    "    Plot the difference between the autoencoder's reconstruction and the original input\n",
    "    Args:\n",
    "        input_fts (numpy array): the original features of the particles\n",
    "        reco_fts (numpy array): the reconstructed features\n",
    "        model_fname (str): name of saved model\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(input_fts, torch.Tensor):\n",
    "        input_fts = input_fts.numpy()\n",
    "    if isinstance(reco_fts, torch.Tensor):\n",
    "        if feature == 'all':\n",
    "            reco_fts = xyze_to_ptetaphi_torch(reco_fts)\n",
    "        reco_fts = reco_fts.numpy()\n",
    "\n",
    "        \n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "  #  label = ['$p_x~[GeV]$', '$p_y~[GeV]$', '$p_z~[GeV]$']\n",
    "   # feat = ['px', 'py', 'pz']\n",
    "    label = ['$p_x~[GeV]$', '$p_y~[GeV]$', '$p_z~[GeV]$']\n",
    "    feat = ['px', 'py', 'pz']\n",
    "    if feature == 'hadronic':# or 'standardized':\n",
    "        label = ['$p_T$', '$eta$', '$phi$']\n",
    "        feat = ['pt', 'eta', 'phi']\n",
    "        \n",
    "    if feature == 'all':# or 'standardized':\n",
    "        label = ['$p_x~[GeV]$', '$p_y~[GeV]$', '$p_z~[GeV]$', '$E~[GeV]$','$p_T$', '$eta$', '$phi$']\n",
    "        feat = ['px', 'py', 'pz','E','pt', 'eta', 'phi']\n",
    "        \n",
    "    # make a separate plot for each feature\n",
    "    for i in range(input_fts.shape[1]):\n",
    "        #plt.style.use(hep.style.CMS)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        if feature == 'cartesian':\n",
    "            bins = np.linspace(-20, 20, 101)\n",
    "            if i == 3:  # different bin size for E momentum\n",
    "                bins = np.linspace(-5, 35, 101)\n",
    "        elif feature == 'hadronic':\n",
    "            bins = np.linspace(-2, 2, 101)\n",
    "            if i == 0:  # different bin size for pt rel\n",
    "                bins = np.linspace(-0.05, 0.1, 101)\n",
    "        elif feature == 'all':\n",
    "            bins = np.linspace(-20, 20, 101)\n",
    "            if i > 3:  # different bin size for hadronic coord\n",
    "                bins = np.linspace(-2, 2, 101)\n",
    "            if i == 3:  # different bin size for E momentum\n",
    "                bins = np.linspace(-5, 35, 101)\n",
    "            if i == 4:  # different bin size for pt rel\n",
    "                bins = np.linspace(-2, 10, 101)\n",
    "        else:\n",
    "            bins = np.linspace(-1, 1, 101)\n",
    "        plt.ticklabel_format(useMathText=True)\n",
    "        plt.hist(input_fts[:,i], bins=bins, alpha=0.5, label='Input', histtype='step', lw=5)\n",
    "        plt.hist(reco_fts[:,i], bins=bins, alpha=0.5, label='Output', histtype='step', lw=5)\n",
    "        plt.legend(title='QCD dataset', fontsize='x-large')\n",
    "        plt.xlabel(label[i], fontsize='x-large')\n",
    "        plt.ylabel('Particles', fontsize='x-large')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(osp.join(save_path, feat[i] + '.pdf'))\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_standardization = False\n",
    "save_dir = '/eos/user/n/nchernya/MLHEP/AnomalyDetection/ADgvae/output_models/pytroch/'\n",
    "plot_reco_for_loader(model, loader, device, scaler, inverse_standardization, 'test_train', osp.join(save_dir, 'reconstruction_post_train', 'train'), 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
