{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import tqdm\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import inspect\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.data import Data, DataLoader, DataListLoader\n",
    "from torch_geometric.nn import EdgeConv, global_mean_pool, DataParallel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data,Dataset\n",
    "from torch_scatter import scatter_mean, scatter\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import MetaLayer, EdgeConv, global_mean_pool, DynamicEdgeConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Samples\n",
    "DATA_PATH = '/eos/cms/store/group/phys_b2g/CASE/h5_files/full_run2/BB_UL_MC_small_v2/'\n",
    "\n",
    "TRAIN_NAME = 'BB_batch0.h5'\n",
    "filename_bg = DATA_PATH + TRAIN_NAME \n",
    "batch_size = 128\n",
    "train_set_size = int((5*10e3//batch_size)*batch_size)\n",
    "#file_bg = h5py.File(filename_bg, 'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "deta_jj = 1.4\n",
    "jPt = 400\n",
    "\n",
    "def xyze_to_eppt(constituents):\n",
    "    ''' converts an array [N x 100, 4] of particles\n",
    "from px, py, pz, E to eta, phi, pt (mass omitted)\n",
    "    '''\n",
    "    PX, PY, PZ, E = range(4)\n",
    "    pt = np.sqrt(np.float_power(constituents[:,PX], 2) + np.float_power(constituents[:,PY], 2), dtype='float32') # numpy.float16 dtype -> float power to avoid overflow\n",
    "    eta = np.arcsinh(np.divide(constituents[:,PZ], pt, out=np.zeros_like(pt), where=pt!=0.), dtype='float32')\n",
    "    phi = np.arctan2(constituents[:,PY], constituents[:,PX], dtype='float32')\n",
    "\n",
    "    return np.stack([pt, eta, phi], axis=1)\n",
    "\n",
    "side = True\n",
    "to_train = True\n",
    "\n",
    "datas = []\n",
    "for i_e in range(1000):\n",
    "    if to_train: \n",
    "        if file_bg['truth_label'][i_e]!=0 : #train only on QCD\n",
    "            continue \n",
    "    if side :\n",
    "        if not (file_bg[\"jet_kinematics\"][i_e,1] > deta_jj):\n",
    "            continue\n",
    "    else : \n",
    "        if not (file_bg[\"jet_kinematics\"][i_e,1] < deta_jj):\n",
    "            continue\n",
    "    for i_j in range(2): #each event has 2 jets\n",
    "        pf_cands = np.array(file_bg[\"jet{}_PFCands\".format(i_j+1)][i_e])\n",
    "        pf_pt_eta_phi = xyze_to_eppt(pf_cands)\n",
    "        n_particles = int(np.sum(pf_pt_eta_phi[:,0]!=0)) #if pt!=0\n",
    "        particles = np.zeros((n_particles, 7)) #px,py,pz,E, pt, eta, phi = 7\n",
    "        #particles = np.dstack((pf_cands[0:n_particles,:],np.array(pf_pt_eta_phi[0:n_particles,:])))\n",
    "        particles = np.hstack((pf_cands[0:n_particles,:],np.array(pf_pt_eta_phi[0:n_particles,:])))\n",
    "        pairs = np.stack([[m, n] for (m, n) in itertools.product(range(n_particles),range(n_particles)) if m!=n])\n",
    "        edge_index = torch.tensor(pairs, dtype=torch.long)\n",
    "        edge_index=edge_index.t().contiguous()\n",
    "        # save particles as node attributes and target\n",
    "        x = torch.tensor(particles, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        datas.append([data])\n",
    "datas = sum(datas,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 10\n",
      "train subset dim: 40\n",
      "validation subset dim 10\n",
      "train_dataloader dim: 8\n",
      "val dataloader dim: 2\n"
     ]
    }
   ],
   "source": [
    "def get_present_constit(x,n):\n",
    "    return x[0:n,:] \n",
    "\n",
    "def concat_features(feats_1,feats_2):\n",
    "    return np.hstack((feats_1[:,:],feats_2[:,:]))\n",
    "\n",
    "class GraphDataset(Dataset):  ####pytorch Dataset\n",
    "    def __init__(self, root, transform=None, pre_transform=None,\n",
    "                 n_events=-1,n_jets=10e3, side_reg=1, features='xyzeptep',n_proc=1):\n",
    "        \"\"\"\n",
    "        Initialize parameters of graph dataset\n",
    "        Args:\n",
    "            root (str): dir path\n",
    "            n_events (int): how many events to process (-1=all in a file (there is a max))\n",
    "            n_jets (int) : how many total jets to use\n",
    "            side_reg (bool):true or false, side region for training, otherwise for testing on signal \n",
    "            n_proc (int): number of processes to split into\n",
    "            features (str): (px, py, pz) or relative (pt, eta, phi)\n",
    "        \"\"\"\n",
    "        max_events = int(1.1e6)\n",
    "        self.n_events = max_events if n_events==-1 else n_events\n",
    "        self.n_jets = n_jets \n",
    "        self.side_reg = side_reg\n",
    "        self.n_proc = n_proc\n",
    "        self.chunk_size = self.n_events // self.n_proc\n",
    "        self.features = features\n",
    "        self.dEtaJJ = 1.4\n",
    "        self.jPt = 400\n",
    "        self.jet_kin_names = ['mJJ', 'DeltaEtaJJ', 'j1Pt', 'j1Eta', 'j1Phi',\\\n",
    "                                        'j1M', 'j2Pt', 'j2Eta', 'j2Phi', 'j2M', 'j3Pt', 'j3Eta', 'j3Phi', 'j3M']\n",
    "        self.pf_kin_names = ['px','py','pz','E']\n",
    "        \n",
    "        super(GraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        self.input_files = sorted(glob.glob(self.raw_dir+'/*.root'))\n",
    "        return [f.split('/')[-1] for f in self.input_files]\n",
    "\n",
    "    def __len__(self):\n",
    "      #  return len(self.n_jets) \n",
    "        return 50\n",
    "  \n",
    "    def xyze_to_ptep(self,constituents):\n",
    "        ''' converts an array [N x 100, 4] of particles\n",
    "from px, py, pz, E to eta, phi, pt (mass omitted)\n",
    "    '''\n",
    "        PX = self.pf_kin_names.index('px')\n",
    "        PY = self.pf_kin_names.index('py')\n",
    "        PZ = self.pf_kin_names.index('pz')\n",
    "        E = self.pf_kin_names.index('E')\n",
    "        pt = np.sqrt(np.float_power(constituents[:,:,PX], 2) + np.float_power(constituents[:,:,PY], 2), dtype='float32') # numpy.float16 dtype -> float power to avoid overflow\n",
    "        eta = np.arcsinh(np.divide(constituents[:,:,PZ], pt, out=np.zeros_like(pt), where=pt!=0.), dtype='float32')\n",
    "        phi = np.arctan2(constituents[:,:,PY], constituents[:,:,PX], dtype='float32')\n",
    "        return np.stack([pt, eta, phi], axis=2)\n",
    "\n",
    "\n",
    "\n",
    "    def read_events(self,idx):\n",
    "        \n",
    "        #Data Samples\n",
    "        DATA_PATH = '/eos/cms/store/group/phys_b2g/CASE/h5_files/full_run2/BB_UL_MC_small_v2/'\n",
    "        TRAIN_NAME = 'BB_batch0.h5'\n",
    "        filename_bg = DATA_PATH + TRAIN_NAME \n",
    "        in_file = h5py.File(filename_bg, 'r') \n",
    "        jet_kin = np.array(in_file[\"jet_kinematics\"])[0:1000]\n",
    "        truth = np.array(in_file[\"truth_label\"])[0:1000]\n",
    "\n",
    "        j1Pt_mask = (jet_kin[:,self.jet_kin_names.index('j1Pt')] > self.jPt)\n",
    "        j2Pt_mask = (jet_kin[:,self.jet_kin_names.index('j2Pt')] > self.jPt)\n",
    "        full_mask = j1Pt_mask & j2Pt_mask\n",
    "        if self.side_reg : \n",
    "            full_mask = full_mask & (jet_kin[:,self.jet_kin_names.index('DeltaEtaJJ')] > self.dEtaJJ)\n",
    "        else : \n",
    "            full_mask = full_mask & (jet_kin[:,self.jet_kin_names.index('DeltaEtaJJ')] < self.dEtaJJ)\n",
    "\n",
    "        #Apply mask on jet kinematics, truth and pf cands\n",
    "        jet_kin = jet_kin[full_mask]\n",
    "        truth = truth[full_mask]\n",
    "        jet_const = [np.array(in_file[\"jet1_PFCands\"][0:1000])[full_mask],np.array(in_file[\"jet2_PFCands\"][0:1000])[full_mask]]\n",
    "                \n",
    "\n",
    "        pf_out_list = []\n",
    "        jet_prop_list = []\n",
    "\n",
    "        for i_j in range(2): #each event has 2 jets\n",
    "            pf_xyze = jet_const[i_j]\n",
    "            pf_ptep = self.xyze_to_ptep(pf_xyze)\n",
    "            n_particles = np.sum(pf_xyze[:,:,self.pf_kin_names.index('E')]!=0,axis=1) #E is 3rd \n",
    "            pf_xyze_out = list(map(get_present_constit,pf_xyze,n_particles))\n",
    "            pf_ptep_out = list(map(get_present_constit,pf_ptep,n_particles))\n",
    "            pf_tot_out = list(map(concat_features,pf_xyze_out,pf_ptep_out))\n",
    "            pf_out_list.append(pf_tot_out)\n",
    "\n",
    "            n_jet_feats = 6\n",
    "            jet_prop = np.zeros((len(pf_tot_out),n_jet_feats))\n",
    "            jet_prop[:,0] = n_particles\n",
    "            for i_f,f_name in enumerate('M,Pt,Eta,Phi'.split(',')):\n",
    "                jet_prop[:,i_f+1] = jet_kin[:,self.jet_kin_names.index('j{}{}'.format(i_j+1,f_name))]\n",
    "            jet_prop[:,n_jet_feats-1] = truth[:,0]\n",
    "            jet_prop_list.append(jet_prop)\n",
    "            \n",
    "        #return list of pf particles, and list of global jet properties\n",
    "        return sum(pf_out_list, []),np.vstack((jet_prop_list[0],jet_prop_list[1]))      \n",
    "                 \n",
    "\n",
    "    def get(self,idx):\n",
    "        '''Yields one data graph'''\n",
    "        print('getting data')\n",
    "        pf_cands, jet_prop = self.read_events(idx)   \n",
    "        \n",
    "        i_evt = idx\n",
    "        #for i_evt in range(len(pf_cands)):\n",
    "        n_particles = pf_cands[i_evt].shape[0]\n",
    "        print('event : ',i_evt,' n particle : ',n_particles)\n",
    "        pairs = np.stack([[m, n] for (m, n) in itertools.product(range(n_particles),range(n_particles)) if m!=n])\n",
    "        edge_index = torch.tensor(pairs, dtype=torch.long)\n",
    "        edge_index=edge_index.t().contiguous()\n",
    "        # save particles as node attributes and target\n",
    "        x = torch.tensor(pf_cands[i_evt], dtype=torch.float)\n",
    "        u = torch.tensor(jet_prop[i_evt,:], dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index,u=torch.unsqueeze(u, 0))\n",
    "        print('data item ',i_evt)\n",
    "        return data\n",
    "        \n",
    "data_dir = '/eos/user/n/nchernya/MLHEP/AnomalyDetection/ADgvae/output_models/pytroch/'\n",
    "dataset = GraphDataset(root=data_dir)\n",
    "    \n",
    "validation_split = 0.2\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "if dataset_size > 2:\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "else: \n",
    "    split = 1\n",
    "print(dataset_size,split)\n",
    "random_seed= 1001\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(dataset, [dataset_size - split, split],\n",
    "                                                             generator=torch.Generator().manual_seed(random_seed))\n",
    "print(\"train subset dim:\", len(train_subset))\n",
    "print(\"validation subset dim\", len(val_subset))\n",
    "dataloaders = {\n",
    "    'train':  DataLoader(train_subset, batch_size=5, shuffle=True),\n",
    "    'val':   DataLoader(val_subset, batch_size=5, shuffle=True)\n",
    "    }\n",
    "print(\"train_dataloader dim:\", len(dataloaders['train']))\n",
    "print(\"val dataloader dim:\", len(dataloaders['val']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data\n",
      "event :  1  n particle :  44\n",
      "data item  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4.4000e+01, 1.1680e+01, 5.4650e+02, 5.7762e-02, 2.6978e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get(1).u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Model definitions.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "from torch_scatter import scatter_mean, scatter\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import MetaLayer, EdgeConv, global_mean_pool, DynamicEdgeConv\n",
    "\n",
    "\n",
    "# GNN AE using EdgeConv (mean aggregation graph operation). Basic GAE model.\n",
    "class EdgeNet(nn.Module):\n",
    "    def __init__(self, input_dim=7, output_dim=4, big_dim=32, hidden_dim=2, aggr='mean'):\n",
    "        super(EdgeNet, self).__init__()\n",
    "        encoder_nn = nn.Sequential(nn.Linear(2*(input_dim), big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        decoder_nn = nn.Sequential(nn.Linear(2*(hidden_dim), big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, big_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(big_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm1d(input_dim)\n",
    "\n",
    "        self.encoder = EdgeConv(nn=encoder_nn,aggr=aggr)\n",
    "        self.decoder = EdgeConv(nn=decoder_nn,aggr=aggr)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.batchnorm(data.x)\n",
    "        x = self.encoder(x,data.edge_index)\n",
    "        x = self.decoder(x,data.edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader, total, batch_size, loss_ftn_obj):\n",
    "    model.train()\n",
    "\n",
    "    sum_loss = 0.\n",
    "    t = tqdm.tqdm(enumerate(loader),total=total/batch_size)\n",
    "    for i,data in t:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss, batch_output = forward_loss(model, data, loss_ftn_obj, device, multi_gpu=False)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = batch_loss.item()\n",
    "        sum_loss += batch_loss\n",
    "        t.set_description('train loss = %.7f' % batch_loss)\n",
    "        t.refresh() # to show immediately the update\n",
    "\n",
    "    return sum_loss / (i+1)\n",
    "\n",
    "\n",
    "# helper to perform correct loss\n",
    "def forward_loss(model, data, loss_ftn_obj, device, multi_gpu=False):\n",
    "    \n",
    "    if not multi_gpu:\n",
    "        data = data.to(device)\n",
    "\n",
    "    if 'emd_loss' in loss_ftn_obj.name or loss_ftn_obj.name == 'chamfer_loss' or loss_ftn_obj.name == 'hungarian_loss':\n",
    "        batch_output = model(data)\n",
    "        if multi_gpu:\n",
    "            data = Batch.from_data_list(data).to(device)\n",
    "        y = data.x\n",
    "        batch = data.batch\n",
    "        batch_loss = loss_ftn_obj.loss_ftn(batch_output, y, batch)\n",
    "\n",
    "    elif loss_ftn_obj.name == 'emd_in_forward':\n",
    "        _, batch_loss = model(data)\n",
    "        batch_loss = batch_loss.mean()\n",
    "\n",
    "    elif loss_ftn_obj.name == 'vae_loss':\n",
    "        batch_output, mu, log_var = model(data)\n",
    "        y = torch.cat([d.x for d in data]).to(device) if multi_gpu else data.x\n",
    "        y = y.contiguous()\n",
    "        batch_loss = loss_ftn_obj.loss_ftn(batch_output, y, mu, log_var)\n",
    "\n",
    "    else:\n",
    "        batch_output = model(data)\n",
    "        y = torch.cat([d.x for d in data]).to(device) if multi_gpu else data.x\n",
    "        y = y.contiguous()\n",
    "        batch_loss = loss_ftn_obj.loss_ftn(batch_output, y)\n",
    "\n",
    "    return batch_loss, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "multi_gpu = False #torch.cuda.device_count()>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardizer:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        :param data: torch tensor\n",
    "        \"\"\"\n",
    "        self.mean = torch.mean(data, dim=0)\n",
    "        self.std = torch.std(data, dim=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data, log_pt=False):\n",
    "        \"\"\"\n",
    "        :param data: torch tensor\n",
    "        :param log_pt: undo log transformation on pt\n",
    "        \"\"\"\n",
    "        inverse = (data * self.std) + self.mean\n",
    "        if log_pt:\n",
    "            inverse[:,0] = (10 ** inverse[:,0]) - 1\n",
    "        return inverse\n",
    "\n",
    "def standardize(train_dataset,log_pt=False):\n",
    "    \"\"\"\n",
    "    standardize dataset and return scaler for inversion\n",
    "    :param train_dataset: list of Data objects\n",
    "    :param valid_dataset: list of Data objects\n",
    "    :param test_dataset: list of Data objects\n",
    "    :param log_pt: log pt before standardization\n",
    "    :return scaler: sklearn StandardScaler\n",
    "    \"\"\"\n",
    "    train_x = torch.cat([d.x for d in train_dataset])\n",
    "    if log_pt:\n",
    "        train_x[:,0] = torch.log(train_x[:,0] + 1)\n",
    "\n",
    "    scaler = Standardizer()\n",
    "    scaler.fit(train_x)\n",
    "    for d in train_dataset:\n",
    "        d.x[:,:] = scaler.transform(d.x)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = DataLoader(datas, batch_size=128)\n",
    "#scaler = standardize(datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyze_to_ptetaphi_torch(y):\n",
    "    ''' converts an array [N x 100, 4] of particles\n",
    "from px, py, pz, E to pt,eta, phi\n",
    "    '''\n",
    "    PX, PY, PZ, E = range(4)\n",
    "    pt = torch.sqrt(torch.pow(y[:,PX], 2) + torch.pow(y[:,PY], 2)) \n",
    "    eta = torch.asinh(torch.where(pt < 10e-5, torch.zeros_like(pt), torch.div(y[:,PZ], pt)))\n",
    "    phi = torch.atan2(y[:,PY], y[:,PX])\n",
    "\n",
    "    relu =  m = nn.ReLU() #inplace=True\n",
    "    y_E_trimmed = relu(y[:,-1]) #trimming E\n",
    "    y_pt_trimmed = relu(pt) #trimming pt\n",
    "    full_y = torch.stack((y[:,0],y[:,1],y[:,2],y_E_trimmed,y_pt_trimmed,eta,phi), dim=1)\n",
    "\n",
    "    return full_y\n",
    "\n",
    "\n",
    "class LossFunction:\n",
    "    def __init__(self, lossname, device=torch.device('cuda:0')):\n",
    "        loss = getattr(self, lossname)\n",
    "        self.name = lossname\n",
    "        self.loss_ftn = loss\n",
    "        self.device = device\n",
    "    def mse(self, x, y):\n",
    "        return F.mse_loss(x, y, reduction='mean')\n",
    "    \n",
    "    def mse_coordinates(self, y,x): #for some reason convension is : out,in\n",
    "        #From px,py,pz,E get pt, eta, phi (do not predict them)\n",
    "        #x is px,py,pz,E,pt,eta,phi\n",
    "        #y is px,py,pz,E\n",
    "        full_y = xyze_to_ptetaphi_torch(y)\n",
    "        return self.mse(x,full_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeNet(\n",
       "  (batchnorm): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (encoder): EdgeConv(nn=Sequential(\n",
       "    (0): Linear(in_features=14, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (5): ReLU()\n",
       "  ))\n",
       "  (decoder): EdgeConv(nn=Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
       "  ))\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss\n",
    "loss_ftn_obj = LossFunction('mse_coordinates', device=device)\n",
    "\n",
    "# model\n",
    "input_dim = 7\n",
    "output_dim = 4\n",
    "big_dim = 32\n",
    "hidden_dim = 2\n",
    "model = EdgeNet(input_dim=input_dim,output_dim=output_dim, big_dim=big_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 10e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, threshold=1e-6)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2346.4899902:   0%|          | 0/10.0 [00:07<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2346.4899902:   0%|          | 0/10.0 [00:07<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2346.4899902:  10%|█         | 1/10.0 [00:07<01:04,  7.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2345.1508789:  10%|█         | 1/10.0 [00:11<01:04,  7.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2345.1508789:  10%|█         | 1/10.0 [00:11<01:04,  7.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2345.1508789:  20%|██        | 2/10.0 [00:11<00:49,  6.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2344.2705078:  20%|██        | 2/10.0 [00:15<00:49,  6.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2344.2705078:  20%|██        | 2/10.0 [00:15<00:49,  6.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2344.2705078:  30%|███       | 3/10.0 [00:15<00:39,  5.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2343.3686523:  30%|███       | 3/10.0 [00:19<00:39,  5.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2343.3686523:  30%|███       | 3/10.0 [00:19<00:39,  5.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2343.3686523:  40%|████      | 4/10.0 [00:19<00:30,  5.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2342.4001465:  40%|████      | 4/10.0 [00:23<00:30,  5.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2342.4001465:  40%|████      | 4/10.0 [00:23<00:30,  5.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2342.4001465:  50%|█████     | 5/10.0 [00:23<00:24,  4.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2341.3344727:  50%|█████     | 5/10.0 [00:28<00:24,  4.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2341.3344727:  50%|█████     | 5/10.0 [00:28<00:24,  4.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2341.3344727:  60%|██████    | 6/10.0 [00:28<00:19,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2340.1481934:  60%|██████    | 6/10.0 [00:32<00:19,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2340.1481934:  60%|██████    | 6/10.0 [00:32<00:19,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2340.1481934:  70%|███████   | 7/10.0 [00:32<00:13,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2338.8247070:  70%|███████   | 7/10.0 [00:36<00:13,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2338.8247070:  70%|███████   | 7/10.0 [00:36<00:13,  4.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2338.8247070:  80%|████████  | 8/10.0 [00:36<00:09,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "Epoch: 00, Training Loss:   2342.7484\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2337.3657227:   0%|          | 0/10.0 [00:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2337.3657227:   0%|          | 0/10.0 [00:04<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2337.3657227:  10%|█         | 1/10.0 [00:04<00:41,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2335.7321777:  10%|█         | 1/10.0 [00:09<00:41,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2335.7321777:  10%|█         | 1/10.0 [00:09<00:41,  4.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2335.7321777:  20%|██        | 2/10.0 [00:09<00:36,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2333.8688965:  20%|██        | 2/10.0 [00:13<00:36,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2333.8688965:  20%|██        | 2/10.0 [00:13<00:36,  4.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2333.8688965:  30%|███       | 3/10.0 [00:13<00:31,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2331.5202637:  30%|███       | 3/10.0 [00:17<00:31,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2331.5202637:  30%|███       | 3/10.0 [00:17<00:31,  4.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2331.5202637:  40%|████      | 4/10.0 [00:17<00:26,  4.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n",
      "data item  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2328.5996094:  40%|████      | 4/10.0 [00:22<00:26,  4.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2328.5996094:  40%|████      | 4/10.0 [00:22<00:26,  4.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train loss = 2328.5996094:  50%|█████     | 5/10.0 [00:22<00:22,  4.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data item  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b65cf97cef83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#loss = train(model, optimizer, loader, len(datas), 128, loss_ftn_obj)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_ftn_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {:02d}, Training Loss:   {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-77ae6e521f70>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loader, total, batch_size, loss_ftn_obj)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/pytorch_geometric/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    185\u001b[0m         dataset at the specified indices.\"\"\"\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-58d3c3fbbe12>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m'''Yields one data graph'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mpf_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_prop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#for i_evt in range(len(pf_cands)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-58d3c3fbbe12>\u001b[0m in \u001b[0;36mread_events\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mfilename_bg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTRAIN_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0min_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mjet_kin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"jet_kinematics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"truth_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mread_direct\u001b[0;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n_epochs = 3\n",
    "stale_epochs = 0\n",
    "loss = 999999\n",
    "train_losses = []\n",
    "for epoch in range(0, n_epochs):\n",
    "    #loss = train(model, optimizer, loader, len(datas), 128, loss_ftn_obj)\n",
    "    loss = train(model, optimizer, loader['train'], 50, 5, loss_ftn_obj)\n",
    "    train_losses.append(loss)\n",
    "    print('Epoch: {:02d}, Training Loss:   {:.4f}'.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gen_in_out(model, loader, device):\n",
    "    model.eval()\n",
    "    input_fts = []\n",
    "    reco_fts = []\n",
    "\n",
    "    for t in loader:\n",
    "        if isinstance(t, list):\n",
    "            for d in t:\n",
    "                input_fts.append(d.x)\n",
    "        else:\n",
    "            input_fts.append(t.x)\n",
    "            t.to(device)\n",
    "\n",
    "        reco_out = model(t)\n",
    "        if isinstance(reco_out, tuple):\n",
    "            reco_out = reco_out[0]\n",
    "        reco_fts.append(reco_out.cpu().detach())\n",
    "\n",
    "    input_fts = torch.cat(input_fts)\n",
    "    reco_fts = torch.cat(reco_fts)\n",
    "    return input_fts, reco_fts\n",
    "\n",
    "def plot_reco_for_loader(model, loader, device, scaler, inverse_scale, model_fname, save_dir, feature_format):\n",
    "    input_fts, reco_fts = gen_in_out(model, loader, device)\n",
    "    if inverse_scale:\n",
    "        input_fts = scaler.inverse_transform(input_fts)\n",
    "        reco_fts = scaler.inverse_transform(reco_fts)\n",
    "    plot_reco_difference(input_fts, reco_fts, model_fname, save_dir, feature_format)\n",
    "\n",
    "    \n",
    "def plot_reco_difference(input_fts, reco_fts, model_fname, save_path, feature='hadronic'):\n",
    "    \"\"\"\n",
    "    Plot the difference between the autoencoder's reconstruction and the original input\n",
    "    Args:\n",
    "        input_fts (numpy array): the original features of the particles\n",
    "        reco_fts (numpy array): the reconstructed features\n",
    "        model_fname (str): name of saved model\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(input_fts, torch.Tensor):\n",
    "        input_fts = input_fts.numpy()\n",
    "    if isinstance(reco_fts, torch.Tensor):\n",
    "        if feature == 'all':\n",
    "            reco_fts = xyze_to_ptetaphi_torch(reco_fts)\n",
    "        reco_fts = reco_fts.numpy()\n",
    "\n",
    "        \n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "  #  label = ['$p_x~[GeV]$', '$p_y~[GeV]$', '$p_z~[GeV]$']\n",
    "   # feat = ['px', 'py', 'pz']\n",
    "    label = ['$p_x~[GeV]$', '$p_y~[GeV]$', '$p_z~[GeV]$']\n",
    "    feat = ['px', 'py', 'pz']\n",
    "    if feature == 'hadronic':# or 'standardized':\n",
    "        label = ['$p_T$', '$eta$', '$phi$']\n",
    "        feat = ['pt', 'eta', 'phi']\n",
    "        \n",
    "    if feature == 'all':# or 'standardized':\n",
    "        label = ['$p_x~[GeV]$', '$p_y~[GeV]$', '$p_z~[GeV]$', '$E~[GeV]$','$p_T$', '$eta$', '$phi$']\n",
    "        feat = ['px', 'py', 'pz','E','pt', 'eta', 'phi']\n",
    "        \n",
    "    # make a separate plot for each feature\n",
    "    for i in range(input_fts.shape[1]):\n",
    "        #plt.style.use(hep.style.CMS)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        if feature == 'cartesian':\n",
    "            bins = np.linspace(-20, 20, 101)\n",
    "            if i == 3:  # different bin size for E momentum\n",
    "                bins = np.linspace(-5, 35, 101)\n",
    "        elif feature == 'hadronic':\n",
    "            bins = np.linspace(-2, 2, 101)\n",
    "            if i == 0:  # different bin size for pt rel\n",
    "                bins = np.linspace(-0.05, 0.1, 101)\n",
    "        elif feature == 'all':\n",
    "            bins = np.linspace(-20, 20, 101)\n",
    "            if i > 3:  # different bin size for hadronic coord\n",
    "                bins = np.linspace(-2, 2, 101)\n",
    "            if i == 3:  # different bin size for E momentum\n",
    "                bins = np.linspace(-5, 35, 101)\n",
    "            if i == 4:  # different bin size for pt rel\n",
    "                bins = np.linspace(-2, 10, 101)\n",
    "        else:\n",
    "            bins = np.linspace(-1, 1, 101)\n",
    "        plt.ticklabel_format(useMathText=True)\n",
    "        plt.hist(input_fts[:,i], bins=bins, alpha=0.5, label='Input', histtype='step', lw=5)\n",
    "        plt.hist(reco_fts[:,i], bins=bins, alpha=0.5, label='Output', histtype='step', lw=5)\n",
    "        plt.legend(title='QCD dataset', fontsize='x-large')\n",
    "        plt.xlabel(label[i], fontsize='x-large')\n",
    "        plt.ylabel('Particles', fontsize='x-large')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(osp.join(save_path, feat[i] + '.pdf'))\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_standardization = False\n",
    "save_dir = '/eos/user/n/nchernya/MLHEP/AnomalyDetection/ADgvae/output_models/pytroch/'\n",
    "plot_reco_for_loader(model, loader, device, scaler, inverse_standardization, 'test_train', osp.join(save_dir, 'reconstruction_post_train', 'train'), 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
